algorithm_name: 'apex-dqn'

# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-algorithms.html#apex
# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-algorithms.html#dqn
# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-training.html#common-parameters
# https://docs.ray.io/en/latest/rllib/rllib-models.html#default-model-config-settings
# https://github.com/ray-project/ray/issues/9425
algorithm:
    ##### Environment #####
    env: 'OceanEnv'
    seed: 2022
    ##### Framework #####
    framework: 'tf2'
    eager_tracing: True
    ##### Episodes #####
    # "batch_mode": "complete_episodes",
    # "horizon": 10000,
    # "soft_horizon": True,
    # "no_done_at_end": False,
    ##### Model #####
    model:
        # _use_default_native_models: True
        # Filter config: List of [out_channels, kernel, stride] for each filter.
#        dim: 21
#        conv_filters: [
#            [4, [5, 5], 1],
#            [16, [5, 5], 1],
#            [441, [21, 21], 1]
#        ]
#        # Supported values are: "tanh", "relu", "swish" (or "silu"), "linear" (or None).
#        conv_activation: 'relu'
#        fcnet_hiddens: [256, 256]
#        fcnet_activation: 'tanh'
#        post_fcnet_hiddens: [256, 256]
#        post_fcnet_activation: 'relu'
        custom_model: 'OceanKerasModel'
        custom_model_config:
###            conv_filters: [
###                [16, [5, 5], 1],
###                [32, [5, 5], 1],
###            ]
            common_units: [ 512, 512, 256 ]
            common_activation: 'tanh'
            common_initializer_std: [ 1, 1, 0.01 ]
#            dueling_units: [ 64, 64 ]
#            dueling_activation: 'relu'
    hiddens: [ 128, 128 ]
    ##### DQN #####
    num_atoms: 1
    n_step: 1
    noisy: False
    dueling: True
    double_q: True
    ##### Training #####
    replay_buffer_config:
        learning_starts: 50000
            # # For now we don't use the new ReplayBuffer API here
            # "_enable_replay_buffer_api": False,
            # "no_local_replay_buffer": True,
            # "type": "MultiAgentReplayBuffer",
            # "capacity": 2000000,
            # "replay_batch_size": 32,
            # "prioritized_replay_alpha": 0.6,
            # # Beta parameter for sampling from prioritized replay buffer.
            # "prioritized_replay_beta": 0.4,
            # # Epsilon to add to the TD errors when updating priorities.
            # "prioritized_replay_eps": 1e-6,
            # # prioritized_replay_alpha: Alpha parameter controls the degree of
            # #     prioritization in the buffer. In other words, when a buffer sample has
            # #     a higher temporal-difference error, with how much more probability
            # #     should it drawn to use to update the parametrized Q-network. 0.0
            # #     corresponds to uniform probability. Setting much above 1.0 may quickly
            # #     result as the sampling distribution could become heavily “pointy” with
            # #     low entropy.
            # #     prioritized_replay_beta: Beta parameter controls the degree of
            # #     importance sampling which suppresses the influence of gradient updates
            # #     from samples that have higher probability of being sampled via alpha
            # #     parameter and the temporal-difference error.
            # #     prioritized_replay_eps: Epsilon parameter sets the baseline probability
            # #     for sampling so that when the temporal-difference error of a sample is
            # #     zero, there is still a chance of drawing the sample.
    train_batch_size: 512
    rollout_fragment_length: 100
#    timesteps_per_iteration: 50000,
#    training_intensity: 2,
        # "target_network_update_freq": 100000,
        # "exploration_config": {"type": "PerWorkerEpsilonGreedy"},
        # "worker_side_prioritization": True,
        # # This will set the ratio of replayed from a buffer and learned
        # # on timesteps to sampled from an environment and stored in the replay
        # # buffer timesteps. Must be greater than 0.
    ##### Workers #####
    num_gpus: 1
    num_workers: 102
    num_cpus_per_worker: 1
    num_gpus_per_worker: 0
    placement_strategy: 'SPREAD'
    ignore_worker_failures: True
    recreate_failed_workers: True

experiments_folder: '/seaweed-storage/experiments/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast/'

environment:
    generation_folder: '/seaweed-storage/generation/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast/fixed_forecast_50000_batches/'
    scenario_name: 'gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast'
    scenario_config: {}

    arena_steps_per_env_step: 1
    actions: 8
    render: False
    fake: False #one of: False, 'random', 'naive, 'hj_planner_forecast', 'hj_planner_hindcast'

feature_constructor:
    measurements: False
    local_map:
        xy_width_degree: 0.2
        xy_width_points: 5
        features:
            ttr_forecast: True
            ttr_hindcast: False
            observer:
                variables: ['error_u', 'error_v'] #['error_u', 'error_v'] #['water_u', 'water_v'], # list from: 'error_u', 'error_v', 'std_error_u', 'std_error_v', 'initial_forecast_u', 'initial_forecast_v', 'water_u', 'water_v'
                time: [0]
            currents_hindcast: [] # offsets in h
            currents_forecast: [] # offsets in h
    global_map: False
    meta: False #['lon', 'lat', 'time', 'target_distance', 'target_direction', 'episode_time']


reward_function:
    delta_ttr_forecast: 0
    delta_ttr_hindcast: 1
    step_punishment: -0.16
    target_bonus: 0
    fail_punishment: 0