algorithm_name: 'apex-dqn'

# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-algorithms.html#apex
# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-algorithms.html#dqn
# https://docs.ray.io/en/releases-1.13.0/rllib/rllib-training.html#common-parameters
# https://docs.ray.io/en/latest/rllib/rllib-models.html#default-model-config-settings
# https://github.com/ray-project/ray/issues/9425
algorithm:
    ##### Environment #####
    env: 'OceanEnv'
    disable_env_checking: True
    seed: 2022
    ##### Framework #####
    framework: 'torch'
    ##### DQN #####
    num_atoms: 1
    n_step: 1
    noisy: False
    dueling: True
    double_q: True
    ##### Model #####
    model:
        custom_model: 'OceanTorchModel'
        custom_model_config:
            map:
                normalize: False

                units:  []
                activations: []
                initializers: []
                dropouts: []

#                units: [ 512, 512, 256 ]
#                activations: [ 'tanh', 'tanh', 'tanh' ] # Supported values: "tanh", "relu", "swish", "linear".
#                initializers: [ 1, 1, 0.01 ] # Supported values: 'xavier_uniform' or float (std of normc_initializer)
#                dropouts: [0, 0, 0]

#                channels: [ 60, 60, 60 ]
#                kernels: [ 3, 3, 3 ]
#                strides: [ 1, 1, 1]
#                paddings: [ 1, 1, 1]
#                poolings: [ null, null, ['max', 3]]
#                groups: [3, 3, 3]
#                dropouts: [0, 0, 0]

            meta:
                input_activation: 'tanh'

                units: []
                activations: [] # Supported values: "tanh", "relu", "swish", "linear".
                initializers: [] # Supported values: 'xavier_uniform' or float (std of normc_initializer)
                dropouts: []

            joined:
                input_activation: False #'tanh'
#
#                units: []
#                activations: [] # Supported values: "tanh", "relu", "swish", "linear".
#                initializers: [] # Supported values: 'xavier_uniform' or float (std of normc_initializer)

#                units: [ 256, 256 ]
#                activations: [ 'tanh', 'tanh' ] # Supported values: "tanh", "relu", "swish", "linear".
#                initializers: [ 1, 0.01 ] # Supported values: 'xavier_uniform' or float (std of normc_initializer)
#                dropouts: [0, 0]

                units: [ 512, 512, 256 ]
                activations: [ 'tanh', 'tanh', 'tanh' ] # Supported values: "tanh", "relu", "swish", "linear".
                initializers: [ 1, 1, 0.01 ] # Supported values: 'xavier_uniform' or float (std of normc_initializer)
                dropouts: [0, 0, 0]

            dueling_heads:
                residual: False
                units: [ 128, 128 ]
                activations: [ 'relu', 'relu', 'linear' ] # Supported values: "tanh", "relu", "swish", "linear".
                initializers: [ 'xavier_uniform', 'xavier_uniform', 'xavier_uniform' ] # Supported values: 'xavier_uniform' or float (std of normc_initializer)
                dropouts: [0, 0, 0]

    _disable_preprocessor_api: True
    ##### Episodes #####
#    batch_mode: 'truncate_episodes'
#    soft_horizon: False
#    rollout_fragment_length: 50
    rollout_fragment_length: 20
    ##### Training #####
    replay_buffer_config:
        capacity: 2000000
#        no_local_replay_buffer: False
#        store_buffer_in_checkpoints: True
#        prioritized_replay_alpha: 0.6
#        prioritized_replay_beta: 0.4
#        prioritized_replay_eps: 0.000001
#        replay_batch_size: 32
#        replay_sequence_length: 1
#        type: 'MultiAgentReplayBuffer'
#    train_batch_size: 512
    train_batch_size: 256
    #    store_buffer_in_checkpoints: True

#    gamma: 0.95 # Loon: 0.93

    min_sample_timesteps_per_iteration: 100000 # 300 steps / epoch * 200 machines = 60000
    target_network_update_freq: 100000
    num_steps_sampled_before_learning_starts: 100000 #500000
    training_intensity: 1

    keep_per_episode_custom_metrics: True
    optimizer:
        num_replay_buffer_shards: 1
    log_level: 'ERROR'
    ##### Exploration #####
#    exploration_config: {"type": "SoftQ"}
        # worker_side_prioritization: True,
    ##### Evaluation #####
    evaluation_interval: 50
    evaluation_duration: 1020
    evaluation_duration_unit: "episodes"
    evaluation_num_workers: 102
    evaluation_sample_timeout_s: 2400
    evaluation_config:
        explore: False
        env_config:
            evaluation: True
    ##### Workers #####
    num_gpus: 1
    num_workers: 102
    num_cpus_per_worker: 0.5
    num_gpus_per_worker: 0
    placement_strategy: 'SPREAD'
    ignore_worker_failures: True
    recreate_failed_workers: True

#    output: 'logdir'

experiments_folder: '/seaweed-storage/experiments/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast/'

environment:
    scenario_file: 'config/reinforcement_learning/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast.yaml'
    scenario_config: {}

    train_missions:
        folder: '/seaweed-storage/generation/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast/divers_training_improved_2022_10_23_05_10_12/'

        filter:
            no_random: True
            stop: 70000

    eval_missions:
        folder: '/seaweed-storage/generation/gulf_of_mexico_Copernicus_forecast_HYCOM_hindcast/divers_training_improved_2022_10_23_05_10_12/'

        filter:
            no_random: True
            start: 70000
            stop: 204

    arena_steps_per_env_step: 6
    actions: 8
    fake: False #one of: False, 'random', 'naive, 'hj_planner_forecast', 'hj_planner_hindcast', 'residual'
    render: False

feature_constructor:
    flatten: False
    measurements: False
    local_map:
        embedding_n: [1, 8, 8]
        embedding_radius: [0, 0.05, 0.1]
#        xy_width_degree: 0.5
#        xy_width_points: 12
        hours: [0] # [0, 4, 8]  # offsets in h
        flatten: False
        features:
            ttr_forecast: True
            ttr_hindcast: False
            # list from: 'mag', 'dir', 'error_u', 'error_v', 'std_error_u', 'std_error_v', 'initial_forecast_u', 'initial_forecast_v', 'water_u', 'water_v'
            observer_variables: [] #['error_u', 'error_v']
            currents_hindcast: False
            currents_forecast: False
            true_error: False
        major: 'time' # one of 'time', 'var'
    global_map: False
    meta: ['lon', 'lat', 'time', 'target_distance', 'target_direction', 'episode_time_in_h', 'hj_fc_direction'] #['lon', 'lat', 'time', 'target_distance', 'target_direction', 'episode_time_in_h', 'hj_fc_direction', 'hj_hc_direction'

reward_function:
    delta_ttr_forecast: 0 # multiplier
    delta_ttr_hindcast: 1 # multiplier
    step_punishment: 0 # 0.16 # substracted flat each step
    target_bonus: 0 # added flat at end if success
    fail_punishment: 0 # substracted flat at end if failed