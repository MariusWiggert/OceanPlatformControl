{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch_filtered = list(filter(lambda x: x is not None, batch))\n",
    "    if not len(batch_filtered):\n",
    "        return None, None\n",
    "    return torch.utils.data.dataloader.default_collate(batch_filtered)\n",
    "\n",
    "\n",
    "def loss_function(prediction, target):\n",
    "    assert prediction.shape == target.shape\n",
    "\n",
    "    return torch.sqrt(F.mse_loss(prediction, target, reduction='mean'))\n",
    "\n",
    "\n",
    "def get_accuracy(outputNN, forecast, target) -> Tuple[float, list[float]]:\n",
    "    assert outputNN.shape == forecast.shape == target.shape\n",
    "    mask = torch.logical_or(torch.isnan(target), target == forecast)\n",
    "    # output_NN[mask] = 0\n",
    "    # target[mask] = 0\n",
    "    # forecast[mask] = 0\n",
    "\n",
    "    magn_NN = torch.sqrt(((outputNN - target) ** 2).nansum(axis=[1, 2, 3, 4]))\n",
    "    magn_initial = torch.sqrt(((forecast - target) ** 2).nansum(axis=[1, 2, 3, 4]))\n",
    "    if (magn_initial == 0).sum():\n",
    "        raise Exception(\"Found Nans! Should not have happened\")\n",
    "\n",
    "    all_ratios = magn_NN / magn_initial\n",
    "\n",
    "    return all_ratios.mean().item(), all_ratios.tolist()\n",
    "\n",
    "\n",
    "def get_optimizer(model, name: str, args_optimizer: dict[str, Any], lr: float):\n",
    "    args_optimizer['lr'] = lr\n",
    "    if name.lower() == \"adam\":\n",
    "        return optim.Adam(model.parameters(), **args_optimizer)\n",
    "    raise warn(\"No optimizer!\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_scheduler(cfg_scheduler, optimizer) -> Tuple[optim.lr_scheduler._LRScheduler, bool]:\n",
    "    name = cfg_scheduler.get(\"name\", \"\")\n",
    "    if name.lower() == \"reducelronplateau\":\n",
    "        print(f\"arguments scheduler: {cfg_scheduler}\")\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(optimizer, **cfg_scheduler.get(\"parameters\", {})), True\n",
    "    raise warn(\"No scheduler!\")\n",
    "    return None, False\n",
    "\n",
    "\n",
    "def get_model(args, cfg_neural_network, device):\n",
    "    model_type = args.model_type\n",
    "    if model_type == 'mlp':\n",
    "        model = OceanCurrentMLP(**cfg_neural_network)\n",
    "    elif model_type == 'cnn':\n",
    "        model = OceanCurrentCNNSubgrid(**cfg_neural_network)\n",
    "    elif model_type == 'rnn':\n",
    "        model = OceanCurrentRNN(**cfg_neural_network)\n",
    "    else:\n",
    "        model = OceanCurrentMLP(**cfg_neural_network)\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def __create_loss_plot(args, train_losses, test_losses, mean_ratio_train, mean_ratio_test):\n",
    "    # Plot the loss and the LR\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(train_losses, color='green', marker='o')\n",
    "    ax.plot(test_losses, color='red', marker='o')\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\", color='green')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(mean_ratio_train, color='lightgreen')\n",
    "    ax2.plot(mean_ratio_test, color='orangered')\n",
    "    ax2.set_ylabel(\"Mean_ratio\", color='blue')\n",
    "    folder = os.path.abspath(\n",
    "        args.get(\"folder_figure\", \"./\") + args[\"model_type\"] + \"/\" + now.strftime(\"%d-%m-%Y_%H-%M-%S\") + \"/\")\n",
    "    filename = f\"_loss_and_lr_{len(mean_ratio_train)}.png\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"saving file {filename} histogram at: {folder}\")\n",
    "    fig.savefig(os.path.join(folder, filename))\n",
    "\n",
    "\n",
    "def __create_histogram(list_ratios: List[float], epoch, args, is_training, n_bins=30):\n",
    "    legend_name = \"training\" if is_training else \"validation\"\n",
    "    plt.figure()\n",
    "    list_ratios = np.array(list_ratios)\n",
    "    list_ratios[list_ratios == np.inf] = 100\n",
    "    plt.hist(list_ratios, bins=n_bins)\n",
    "\n",
    "    plt.axvline(x=1, color='b', label='x=1')\n",
    "    plt.title(\n",
    "        f\"Histogram for {legend_name} at epoch {epoch} with mean {list_ratios.mean():.2f}, std: {list_ratios.std():.2f}\")\n",
    "    plt.xlabel(\"ratio rmse(NN)/rmse(FC)\")\n",
    "    plt.ylabel(f\"frequency (over {len(list_ratios)} samples)\")\n",
    "\n",
    "    folder = os.path.abspath(\n",
    "        args.get(\"folder_figure\", \"./\") + args[\"model_type\"] + \"/\" + now.strftime(\"%d-%m-%Y_%H-%M-%S\") + \"/\")\n",
    "    filename = f'epoch{epoch}_{legend_name}_loss{(f\"{list_ratios.mean():.2f}\").replace(\".\", \"_\")}.png'\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"saving file {filename} histogram at: {folder}\")\n",
    "    plt.savefig(os.path.join(folder, filename))\n",
    "    plt.close()\n",
    "\n",
    "    # wandb histogram\n",
    "    # data = [[i, ratio] for i, ratio in enumerate(list_ratios)]\n",
    "    # fields = {\"x\": \"sample\",\n",
    "    #           \"value\": \"ratios\"}\n",
    "    # # table = wandb.Table(data=data, columns=fields)\n",
    "    # # wandb.log({'histogram_ratio_rmses': wandb.plot.histogram(table, \"ratios\", title=\"Ratio NN vs FC Distribution\")})\n",
    "    # # Use the table to populate the new custom chart preset\n",
    "    # # To use your own saved chart preset, change the vega_spec_name\n",
    "    # my_custom_chart = wandb.plot_table(vega_spec_name=\"carey/new_chart\",\n",
    "    #                                    data_table=table,\n",
    "    #                                    fields=fields,\n",
    "    #                                    )\n",
    "    # # Log the plot to have it show up in the UI\n",
    "    # wandb.log({\"custom_chart\": my_custom_chart})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader: torch.utils.data.DataLoader, optimizer: torch.optim.Optimizer, epoch: int,\n",
    "          model_error: bool,\n",
    "          cfg_dataset: dict[str, any]):\n",
    "    model.train()\n",
    "    with torch.autograd.set_detect_anomaly(True):\n",
    "        # for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n",
    "        total_loss = 0\n",
    "        all_ratios = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                if (data, target) == (None, None):\n",
    "                    continue\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # todo: adapt in case of window\n",
    "                axis = cfg_dataset[\"index_axis_time\"]\n",
    "                shift_input = cfg_dataset.get(\"shift_window_input\", 0)\n",
    "                # We take the matching input timesteps with the output timesteps\n",
    "                data_same_time = torch.moveaxis(\n",
    "                    torch.moveaxis(data, axis, 0)[shift_input:shift_input + target.shape[2]], 0, axis)\n",
    "                # data_same_time = data.select(axis, 0).unsqueeze(axis)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                if model_error:\n",
    "                    output = data_same_time - output\n",
    "                loss = loss_function(output, target)\n",
    "                total_loss += loss.item()\n",
    "                # Backprop\n",
    "                loss.backward()\n",
    "                # update the weights\n",
    "                optimizer.step()\n",
    "                ratio, list_ratios = get_accuracy(output,\n",
    "                                                  data_same_time,\n",
    "                                                  target)\n",
    "                all_ratios += list_ratios\n",
    "                tepoch.set_postfix(loss=loss.item(), mean_ratio=ratio)\n",
    "            # wandb.log({'epoch_loss': total_loss / len(train_loader.dataset), })\n",
    "        total_loss /= len(train_loader)\n",
    "        __create_histogram(all_ratios, epoch, args, True)\n",
    "        print(f\"Training loss: {loss}\")\n",
    "        print(f\"percentage of ratios <= 1: {((np.array(list_ratios) <= 1).sum() / len(list_ratios) * 100):.4f}%, \",\n",
    "              (np.array(list_ratios)))\n",
    "\n",
    "        return total_loss, np.array(list_ratios).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader: torch.utils.data.DataLoader, epoch: int,\n",
    "         model_error: bool, cfg_dataset: dict[str, any]) -> float:\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    initial_loss = 0\n",
    "    accuracy = 0\n",
    "    list_ratios = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "            for data, target in tepoch:\n",
    "                if (data, target) == (None, None):\n",
    "                    continue\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                axis = cfg_dataset[\"index_axis_time\"]\n",
    "                shift_input = cfg_dataset.get(\"shift_window_input\", 0)\n",
    "                # We take the matching input timesteps with the output timesteps\n",
    "                data_same_time = torch.moveaxis(\n",
    "                    torch.moveaxis(data, axis, 0)[shift_input:shift_input + target.shape[2]], 0, axis)\n",
    "                output = model(data)\n",
    "                if model_error:\n",
    "                    output = data_same_time - output\n",
    "                loss = loss_function(output, target).item()  # sum the batch losses\n",
    "                test_loss += loss\n",
    "                ratio, all_ratios = get_accuracy(output,\n",
    "                                                 data_same_time,\n",
    "                                                 target)\n",
    "                list_ratios += all_ratios\n",
    "                # TODO: change that\n",
    "                accuracy += ratio\n",
    "                initial_loss += loss_function(data_same_time, target).item()\n",
    "                tepoch.set_postfix(loss=loss, mean_ratio=ratio)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    initial_loss /= len(test_loader)\n",
    "    accuracy /= len(test_loader)\n",
    "\n",
    "    __create_histogram(all_ratios, epoch, args, False)\n",
    "    print(f\"percentage of ratios <= 1: {((np.array(list_ratios) <= 1).sum() / len(list_ratios) * 100):.4f}%\")\n",
    "\n",
    "    print(\n",
    "        f\"Test set: Average loss: {test_loss:.6f}, Without NN: {initial_loss:.6f}\\n\"\n",
    "        f\"mean ratio SUM(rmse(NN(FC_xt))/rmse(FC_xt)):({accuracy:.6f})\\n\"\n",
    "        f\"Percentage increase: {((initial_loss - test_loss) / initial_loss):.3f}\")\n",
    "    return test_loss, np.array(list_ratios).mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def end_training(model, args, train_losses, validation_losses, train_ratios, validation_ratios):\n",
    "    train_losses = np.array(train_losses)\n",
    "    validation_losses = np.array(validation_losses)\n",
    "    train_ratios = np.array(train_ratios)\n",
    "    validation_ratios = np.array(validation_ratios)\n",
    "    __create_loss_plot(args, train_losses, validation_losses, train_ratios, validation_ratios)\n",
    "\n",
    "    print(\n",
    "        f\"Training over. Best validation loss {validation_ratios.min()} at epoch {validation_ratios.argmin()}\\n\"\n",
    "        f\" with losses train:{train_losses[validation_ratios.argmin()]} test:{validation_losses[validation_ratios.argmin()]}.\\n\"\n",
    "        f\" List of all the training ratios: {train_losses}\")\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), f\"{args.model_type}.pt\")\n",
    "\n",
    "    # Log the summary metric using the test set\n",
    "    # wandb.summary['test_accuracy'] = ...\n",
    "    wandb.summary['best_validation_ratio'] = validation_ratios.min()\n",
    "    wandb.finish()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_args(all_cfgs):\n",
    "    # Training settings\n",
    "    # parser = argparse.ArgumentParser(description='PyTorch model')\n",
    "    # parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "    #                     help='input batch size for training (default: 64)')\n",
    "    # parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "    #                     help='input batch size for testing (default: 1000)')\n",
    "    # parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "    #                     help='number of epochs to train (default: 14)')\n",
    "    # parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "    #                     help='learning rate (default: 1.0)')\n",
    "    # parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "    #                     help='Learning rate step gamma (default: 0.7)')\n",
    "    # parser.add_argument('--yaml-file-datasets', type=str, default='',\n",
    "    #                     help='filname of the yaml file to use to download the data in the folder scenarios/neural_networks')\n",
    "    # parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "    #                     help='disables CUDA training')\n",
    "    # parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "    #                     help='quickly check a single pass')\n",
    "    # parser.add_argument('--silicon', action='store_true', default=False,\n",
    "    #                     help='enable Mac silicon optimization')\n",
    "    # parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "    #                     help='random seed (default: 1)')\n",
    "    # parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "    #                     help='how many batches to wait before logging training status')\n",
    "    # parser.add_argument('--save-model', action='store_true', default=False,\n",
    "    #                     help='For Saving the current Model')\n",
    "    # parser.add_argument('--model-type', type=str, default='mlp')\n",
    "    #\n",
    "    # parser.add_argument('--max-batches-training-set', type=int, default=-1)\n",
    "    # parser.add_argument('--max-batches-validation-set', type=int, default=-1)\n",
    "    # args = parser.parse_args()\n",
    "    # cfgs = yaml.load(open(os.getcwd() + \"/config/\" + args.file_configs + \".yaml\", 'r'), Loader=yaml.FullLoader)\n",
    "\n",
    "    # ALTERNATIVE:\n",
    "    args = all_cfgs.get(\"arguments_model_runner\", {})\n",
    "    args.setdefault(\"batch_size\", 64)\n",
    "    args.setdefault(\"test_batch_size\", 1000)\n",
    "    args.setdefault(\"epochs\", 14)\n",
    "    args.setdefault(\"lr\", 1.0)\n",
    "    args.setdefault(\"gamma\", 0.7)\n",
    "    args.setdefault(\"yaml_file_datasets\", \"\")\n",
    "    args.setdefault(\"no_cuda\", False)\n",
    "    args.setdefault(\"dry_run\", False)\n",
    "    args.setdefault(\"silicon\", False)\n",
    "    args.setdefault(\"seed\", 1)\n",
    "    args.setdefault(\"log_interval\", 10)\n",
    "    args.setdefault(\"save_model\", False)\n",
    "    args.setdefault(\"model_type\", \"mlp\")\n",
    "    args.setdefault(\"max_batches_training_set\", -1)\n",
    "    args.setdefault(\"max_batches_validation_set\", -1)\n",
    "    return DotDict(args), all_cfgs\n",
    "    # END ALTERNATIVE\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.init(project=\"Seaweed_forecast_improvement\", entity=\"killian2k\")  # , name=f\"experiment_{}\")\n",
    "print(f\"starting run: {wandb.run.name}\")\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"Seaweed_forecast_improvement\"\n",
    "# gc.collect()\n",
    "parser = argparse.ArgumentParser(description='yaml config file path')\n",
    "parser.add_argument('--file-configs', type=str, help='name file config to run (without the extension)')\n",
    "config_file = parser.parse_args().file_configs + \".yaml\"\n",
    "all_cfgs = yaml.load(open(config_file, 'r'),\n",
    "                     Loader=yaml.FullLoader)\n",
    "args, all_cfgs = get_args(all_cfgs)\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = device\n",
    "print(\"device:\", device)\n",
    "if args.silicon:\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "cfg_model = all_cfgs.get(\"model\", {})\n",
    "cfg_neural_network = cfg_model.get(\"cfg_neural_network\", {}) | {\"device\": device}\n",
    "cfg_dataset = cfg_model.get(\"cfg_dataset\", {})\n",
    "cfg_optimizer = args.get(\"optimizer\", {})\n",
    "cfg_scheduler = args.get(\"scheduler\", {})\n",
    "cfg_data_generation = all_cfgs.get(\"data_generation\", {})\n",
    "model_error = cfg_model.get(\"model_error\", True)\n",
    "print(\"The Model will predict the \" + (\"error\" if model_error else \"hindcast\") + \".\")\n",
    "folder_training = cfg_data_generation[\"parameters_input\"][\"folder_training\"]\n",
    "folder_validation = cfg_data_generation[\"parameters_input\"][\"folder_validation\"]\n",
    "if folder_training == folder_validation:\n",
    "    warn(\"Training and validation use the same dataset!!!\")\n",
    "\n",
    "train_kwargs = {'batch_size': args.batch_size,\n",
    "                'shuffle': cfg_data_generation['parameters_input'].get('shuffle_training', True)}\n",
    "test_kwargs = {'batch_size': args.test_batch_size,\n",
    "               'shuffle': cfg_data_generation['parameters_input'].get('shuffle_validation', True)}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "wandb.config.update(args, allow_val_change=True)\n",
    "wandb.save(config_file)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Takes a lot of time to load, avoid to do it too often!!!\n",
    "dataset_training = CustomOceanCurrentsFromFiles(folder_training,\n",
    "                                                max_items=args.batch_size * args.max_batches_training_set)\n",
    "dataset_validation = CustomOceanCurrentsFromFiles(folder_validation,\n",
    "                                                  max_items=args.batch_size * args.max_batches_validation_set)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_training, collate_fn=collate_fn, **train_kwargs)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset_validation, collate_fn=collate_fn, **test_kwargs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "model = get_model(args, cfg_neural_network, device)\n",
    "optimizer = get_optimizer(model, cfg_optimizer.get(\"name\", \"\"), cfg_optimizer.get(\"parameters\", {}), args.lr)\n",
    "\n",
    "# scheduler = schedulers.StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "scheduler, scheduler_step_takes_argument = get_scheduler(cfg_scheduler, optimizer)\n",
    "print(f\"optimizer: {optimizer}\")\n",
    "\n",
    "train_ratios, test_ratios = list(), list()\n",
    "train_losses, test_losses = list(), list()\n",
    "try:\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        metrics = {}\n",
    "\n",
    "        # Training\n",
    "        print(f\"starting Training epoch {epoch}/{args.epochs}.\")\n",
    "        time.sleep(0.2)\n",
    "        loss, ratio = train(args, model, device, train_loader, optimizer, epoch,\n",
    "                            model_error, cfg_dataset)\n",
    "        train_losses.append(loss)\n",
    "        train_ratios.append(ratio)\n",
    "        metrics |= {\"train_loss\": loss, \"train_ratio\": ratio}\n",
    "\n",
    "        # Testing\n",
    "        print(f\"starting Testing epoch {epoch}/{args.epochs}.\")\n",
    "        time.sleep(0.2)\n",
    "        loss, ratio = test(args, model, device, validation_loader, epoch,\n",
    "                           model_error, cfg_dataset)\n",
    "        test_losses.append(loss)\n",
    "        test_ratios.append(ratio)\n",
    "        metrics |= {\"test_loss\": loss, \"test_ratio\": ratio, \"learning rate\": optimizer.param_groups[0]['lr']}\n",
    "        if scheduler is not None:\n",
    "            if scheduler_step_takes_argument:\n",
    "                scheduler.step(loss)\n",
    "                print(f\"current lr: {optimizer.param_groups[0]['lr']}\")\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        if epoch % 30 == 0:\n",
    "            __create_loss_plot(args, train_losses, test_losses, train_ratios, test_ratios)\n",
    "finally:\n",
    "    end_training(model, args, train_losses, test_losses, train_ratios, test_ratios)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}